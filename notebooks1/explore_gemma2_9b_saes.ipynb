{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython # type: ignore\n",
    "ipython = get_ipython()\n",
    "assert ipython is not None\n",
    "\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "from datasets import load_dataset  \n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import gc\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sae.sparse_autoencoder import load_saved_sae\n",
    "from sae.metrics import model_store_from_sae\n",
    "from unlearning.metrics import convert_wmdp_data_to_prompt, convert_list_of_dicts_to_dict_of_lists\n",
    "from unlearning.tool import UnlearningConfig, SAEUnlearningTool, MCQ_ActivationStoreAnalysis, ActivationStoreAnalysis\n",
    "from unlearning.metrics import modify_and_calculate_metrics, calculate_metrics_list, create_df_from_metrics, calculate_MCQ_metrics, calculate_metrics_side_effects\n",
    "\n",
    "from unlearning.var import GEMMA_INST_FORMAT\n",
    "from itertools import permutations\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "import json\n",
    "\n",
    "from unlearning.feature_attribution import find_topk_features_given_prompt, test_topk_features\n",
    "\n",
    "\n",
    "all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can skip this if you edited `pretrained_saes.yaml` manually\n",
    "\n",
    "os.environ[\"GEMMA_2_SAE_WEIGHTS_ROOT\"] = \"/workspace/weights/\"\n",
    "assert os.path.exists(os.environ[\"GEMMA_2_SAE_WEIGHTS_ROOT\"])\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemmascope-9b-pt-res\",\n",
    "    sae_id=\"layer_20/width_16k/average_l0_20\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"cais/wmdp\", \"wmdp-bio\")\n",
    "\n",
    "answers = [x['answer'] for x in dataset['test']]\n",
    "questions = [x['question'] for x in dataset['test']]\n",
    "choices_list = [x['choices'] for x in dataset['test']]\n",
    "\n",
    "prompts = [convert_wmdp_data_to_prompt(question, choices, prompt_format=\"GEMMA_INST_FORMAT\") for question, choices in zip(questions, choices_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a278ace1e6746ecb5a5d2aba948af07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained_no_processing(\"google/gemma-2-9b-it\", dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.cfg.hook_point = sae.cfg.hook_name\n",
    "correct_question_ids = np.genfromtxt(\"../data/wmdp-bio_gemma2_9b_it_correct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 0 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 1 2\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:30<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 3 3\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:25<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 7 4\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:22<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 15 5\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:23<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 16 6\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:26<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 17 7\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:31<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 19 8\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:35<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 39 9\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:24<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question # 41 10\n",
      "dict_keys(['blocks.20.hook_resid_post', 'blocks.20.hook_resid_post_grad'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:18<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "feature_per_prompt = {}\n",
    "\n",
    "known_good_features = []\n",
    "\n",
    "for j, question_id in enumerate(correct_question_ids[:10]):\n",
    "\n",
    "    question_id = int(question_id)\n",
    "\n",
    "    print(\"Question #\", question_id, j+1)\n",
    "    \n",
    "    prompt = prompts[question_id]\n",
    "    choices = choices_list[question_id]\n",
    "    answer = answers[question_id]\n",
    "    question = questions[question_id]\n",
    "\n",
    "    topk_features_unique, feature_attributions, topk_features, all_feature_activations, logit_diff_grad, topk_feature_attributions = find_topk_features_given_prompt(model,\n",
    "                                                           prompt,\n",
    "                                                           question,\n",
    "                                                           choices,\n",
    "                                                           answer,\n",
    "                                                           sae,\n",
    "                                                           hook_point=sae.cfg.hook_point)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    intervention_results, feature_ids_to_probs, good_features = test_topk_features(model,\n",
    "                                                                                   sae,\n",
    "                                                                                   question_id,\n",
    "                                                                                   topk_features_unique[:20],\n",
    "                                                                                   known_good_features=known_good_features,\n",
    "                                                                                   multiplier=30,\n",
    "                                                                                   permutations=all_permutations)\n",
    "\n",
    "    feature_per_prompt[question_id] = good_features\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    known_good_features = list(set([item for sublist in feature_per_prompt.values() for item in sublist]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(known_good_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [12165, 15471, 13624, 969, 10034],\n",
       " 1: [1642],\n",
       " 3: [11596, 11933, 11339, 358, 3324, 3633, 7012, 3813],\n",
       " 7: [5847, 1833, 5561, 13982, 8783],\n",
       " 15: [8290, 35],\n",
       " 16: [],\n",
       " 17: [15245, 13051, 10839, 5295, 12501, 1022, 6051, 3655, 12176],\n",
       " 19: [6337, 6266, 1867],\n",
       " 39: [1426, 2962],\n",
       " 41: [2348, 14082]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_per_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [832, 12165, 15471, 969, 10034, 12758]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
