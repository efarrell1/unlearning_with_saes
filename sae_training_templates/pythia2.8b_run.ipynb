{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85b5fd9-82c4-41cd-8074-f52610ef2f53",
   "metadata": {},
   "source": [
    "### Runs for Gemma2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b43f580-8459-4f18-9e4f-8a8748a74000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sae.train import ModelTrainer\n",
    "from sae.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f2c49f-7f83-4cbe-8aeb-e01f7976fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_inputs = {\n",
    "    # Model and Hook Point\n",
    "    'model_name': 'gemma-2b-it',\n",
    "    'hook_point': 'blocks.12.hook_resid_pre',\n",
    "    'hook_point_layer': 12,\n",
    "    'hook_point_head_index': None,\n",
    "    'd_in': 2048,\n",
    "\n",
    "    # Dataset\n",
    "    'dataset': 'Skylion007/openwebtext',\n",
    "    'is_dataset_tokenized': False,\n",
    "    \n",
    "     # Activation Store Parameters\n",
    "    'n_batches_in_store_buffer': 128,\n",
    "    'store_batch_size': 2,\n",
    "    'train_batch_size': 4096,\n",
    "    'context_size': 1024,\n",
    "\n",
    "    # Outputs\n",
    "    'log_to_wandb': True,\n",
    "    'wandb_project': 'test_gemma_2b',\n",
    "    'wandb_log_frequency': 10,\n",
    "    'eval_frequency': 500,\n",
    "    'sparsity_log_frequency': 5000,\n",
    "    'n_checkpoints': 160,\n",
    "    'checkpoint_path': '../outputs/checkpoints',\n",
    "\n",
    "    # Sparse Autoencoder Parameters\n",
    "    'expansion_factor': 8,\n",
    "    'normalise_w_dec': True,\n",
    "    'clip_grad_norm': False,\n",
    "    'scale_input_norm': False,\n",
    "\n",
    "    # General\n",
    "    'seed': 42,\n",
    "    'total_training_steps': 200000,\n",
    "\n",
    "    # Learning rate parameters\n",
    "    'lr': 0.0004,\n",
    "    'lr_scheduler_name': 'constant',\n",
    "\n",
    "    # Loss Function\n",
    "    'mse_loss_coefficient': 1,\n",
    "    'mse_loss_type': 'centered',\n",
    "    'l0_coefficient': 7e-5,\n",
    "    'epsilon_l0_approx': 0.2,\n",
    "    \n",
    "    'sparse_loss_coefficient': 1e-6,\n",
    "    'min_sparsity_target': 1e-5,\n",
    "    \n",
    "    # # Sparse Autoencoder Parameters\n",
    "    # 'expansion_factor': 8,\n",
    "    # 'normalise_initial_decoder_weights': True,\n",
    "    # 'initial_decoder_norm': 0.1,\n",
    "    # 'initialise_encoder_to_decoder_transpose': True,\n",
    "\n",
    "    # 'normalise_w_dec': False,\n",
    "    # 'clip_grad_norm': True,\n",
    "    # 'scale_input_norm': False,\n",
    "\n",
    "    # # General\n",
    "    # 'seed': 42,\n",
    "    # 'total_training_steps': 200000,\n",
    "\n",
    "    # # Learning rate parameters\n",
    "    # 'lr': 1e-4,\n",
    "    # 'lr_scheduler_name': 'constant_with_warmup',\n",
    "    # 'lr_warm_up_steps': 1000,\n",
    "\n",
    "    # # Loss Function\n",
    "    # 'mse_loss_coefficient': 1,\n",
    "    # 'l1_coefficient': 10,\n",
    "    # 'weight_l1_by_decoder_norms': True,\n",
    "    # # 'custom_loss': 'l0_anthropic',\n",
    "    # # 'epsilon_l0_approx': 0.5,\n",
    "\n",
    "    # 'sparse_loss_coefficient': 1e-6,\n",
    "    # 'min_sparsity_target': 1e-5,\n",
    "    \n",
    "    # # Warm up loss coefficients\n",
    "    # 'l1_warmup': True,\n",
    "    # 'l1_warmup_steps': 10000,\n",
    "}\n",
    "\n",
    "cfg = Config(**config_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3aaf77-f26f-4b47-9c4f-e79da33af142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac49a678c0d99cf7e83c664720738bec720cd3bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a67443-c251-49cf-b528-52d96a4cf0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c228b138-28c4-4355-99d1-2ff8ac3b9f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d755631c1115495395f224392f0c741a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "creating activation store\n",
      "creating data loader\n",
      "buffer\n",
      "dataloader\n",
      "creating sae\n",
      "creating wanbd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/unlearning/sae_training_templates/wandb/run-20240701_062440-neip52xr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eoin/test_gemma_2b/runs/neip52xr' target=\"_blank\">ancient-moon-42</a></strong> to <a href='https://wandb.ai/eoin/test_gemma_2b' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eoin/test_gemma_2b' target=\"_blank\">https://wandb.ai/eoin/test_gemma_2b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eoin/test_gemma_2b/runs/neip52xr' target=\"_blank\">https://wandb.ai/eoin/test_gemma_2b/runs/neip52xr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea019772cf934ab6aeab5c6c1aa0e1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_5115904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_10235904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_15355904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_20475904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_25595904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_30715904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_35835904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_40955904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_46075904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_51195904.pt\n",
      "[28785]\n",
      "Saved model to ../outputs/checkpoints/ancient-moon-42/sparse_autoencoder_gemma-2b-it_blocks.12.hook_resid_pre_s16384_56315904.pt\n",
      "[28785]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mod \u001b[38;5;241m=\u001b[39m ModelTrainer(cfg)\n\u001b[1;32m      2\u001b[0m mod\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/unlearning/sae_training_templates/../sae/train.py:149\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03mMain training loop\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_training_steps \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_training_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_training_steps)):\n\u001b[0;32m--> 149\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    150\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input_activations()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mod = ModelTrainer(cfg)\n",
    "mod.setup()\n",
    "mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c73377-b346-49f7-a3e0-cbf631254eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab5b97-ed88-4e2f-a19f-42440c955f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fab007-1459-4ef1-abef-64ed3fcb8423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d5174-1b43-4b67-8dd3-ee3b1bb34313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bb716-4c09-4e0c-aa79-09a33b1292ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b2f39-369b-4702-a9d8-7a570888a22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351ef15-2f5f-459e-92eb-20fd1e664d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9dcf2-aec0-4178-9116-e977802845e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7d8bd-5337-444b-994f-ff273616c17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50496870-042a-4369-afe8-81ead6ba9589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147666a-28d8-42a8-9b10-c677ee95eed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa964f91-a294-41c8-8fe6-9ec84bb0e422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac9d9c-56cf-4746-b490-03c50c22679f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf368a-3714-452b-9e5f-7c817cbf4eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
