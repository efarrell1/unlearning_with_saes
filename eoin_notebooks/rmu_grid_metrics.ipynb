{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f5abda8ad90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sae.sparse_autoencoder import *\n",
    "from sae.activation_store import *\n",
    "from sae.train import ModelTrainer\n",
    "from sae.config import create_config, Config\n",
    "from sae.metrics import *\n",
    "from sae.utils import get_blog_checkpoint, get_blog_sparsity, create_lineplot_histogram\n",
    "from unlearning.metrics import *\n",
    "from unlearning.metrics import calculate_wmdp_bio_metrics\n",
    "\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from sae.metrics import compute_metrics_post_by_text\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "import time\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loss_added(model,\n",
    "                   activation_store,\n",
    "                   n_batch=2):\n",
    "    \n",
    "    activation_store.iterable_dataset = iter(activation_store.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_diffs = []\n",
    "        per_token_loss_diffs = []\n",
    "        token_list = []\n",
    "        \n",
    "        for _ in tqdm(range(n_batch)):\n",
    "            \n",
    "            tokens = activation_store.get_batch_tokenized_data()\n",
    "\n",
    "            logits = model(tokens, return_type=\"logits\")\n",
    "            loss_per_token = get_per_token_loss(logits, tokens)\n",
    "\n",
    "            per_token_loss_diff = loss_per_token\n",
    "\n",
    "            per_token_loss_diffs.append(per_token_loss_diff)\n",
    "            token_list.append(tokens)\n",
    "        \n",
    "        return torch.vstack(per_token_loss_diffs), torch.vstack(token_list)\n",
    "    \n",
    "def get_per_token_loss(logits, tokens):\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    # Use torch.gather to find the log probs of the correct tokens\n",
    "    # Offsets needed because we're predicting the NEXT token (this means the final logit is meaningless)\n",
    "    # None and [..., 0] needed because the tensor used in gather must have the same rank.\n",
    "    predicted_log_probs = log_probs[..., :-1, :].gather(dim=-1, index=tokens[..., 1:, None])[..., 0]\n",
    "    return -predicted_log_probs\n",
    "    \n",
    "def get_loss_added_rmu_model(rmu_model,\n",
    "                             base_model,\n",
    "                             activation_store,\n",
    "                             n_batch=2):\n",
    "    \n",
    "    activation_store.iterable_dataset = iter(activation_store.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_diffs = []\n",
    "        per_token_loss_diffs = []\n",
    "        token_list = []\n",
    "        \n",
    "        for _ in tqdm(range(n_batch)):\n",
    "            \n",
    "            tokens = activation_store.get_batch_tokenized_data()\n",
    "\n",
    "            base_logits = base_model(tokens, return_type=\"logits\")\n",
    "            base_loss_per_token = get_per_token_loss(base_logits, tokens)\n",
    "            # base_loss = base_model(tokens, return_type=\"loss\")\n",
    "            # base_loss_per_token = utils.lm_cross_entropy_loss(base_logits, tokens, per_token=True)\n",
    "\n",
    "            rmu_logits = rmu_model(tokens, return_type=\"logits\")\n",
    "            rmu_loss_per_token = get_per_token_loss(rmu_logits, tokens)\n",
    "            # rmu_loss = rmu_model(tokens, return_type=\"loss\")\n",
    "            # rmu_loss_per_token = utils.lm_cross_entropy_loss(rmu_logits, tokens, per_token=True)\n",
    "\n",
    "            loss_diff = rmu_loss_per_token.mean() -  base_loss_per_token.mean()\n",
    "            per_token_loss_diff = rmu_loss_per_token - base_loss_per_token\n",
    "\n",
    "            loss_diffs.append(loss_diff)\n",
    "            per_token_loss_diffs.append(per_token_loss_diff)\n",
    "            token_list.append(tokens)\n",
    "        \n",
    "        return torch.tensor(loss_diffs), torch.vstack(per_token_loss_diffs), torch.vstack(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2705a71248f2436eadbb7cbf88e442b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"starting\")\n",
    "\n",
    "tlens_model_name = \"google/gemma-2b-it\"\n",
    "base_model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype='auto') #.to(\"cuda\")\n",
    "base_model = HookedTransformer.from_pretrained(tlens_model_name, hf_model=tmodel)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer\n",
      "dataloader\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# resid pre 9\n",
    "REPO_ID = \"eoinf/unlearning_saes\"\n",
    "FILENAME = \"jolly-dream-40/sparse_autoencoder_gemma-2b-it_blocks.9.hook_resid_pre_s16384_127995904.pt\"\n",
    "\n",
    "filename = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "sae = load_saved_sae(filename)\n",
    "\n",
    "activation_store = ActivationStoreAnalysis(sae.cfg, base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48ca04def954c8bb7d92ec363296a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:06<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:23<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gemma_2b_it_rmu_6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48adaebe99854cc8ad4c90d54612af9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:06<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:23<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gemma_2b_it_rmu_10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08dad10341c94f6783656ca05143a9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:06<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gemma_2b_it_rmu_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b063c341f8a47589cadcfc2a301b3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f8dff982234769a5a1a723d5d5506e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f5083262474083a5da77fa99363f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5722a8cd3c1143a9a4e7bffa6d92514e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f16f39dfcd45ef80ce0e25b4b4b6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648ae68704f2476eb57e7a4b8f19bc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab9026a672a4b25b1afeea6404587a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:06<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gemma_2b_it_rmu_60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e865f34de249cfbbc060ab8d630e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5541c190ec944cf5ba5d7924c35a63d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bfe1c1bf634a8eb19ece6c79f70331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b059993dffe04f8caee0df6ff81f4fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be8505915834534b3d4335680c821f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6665a28cb463458096312e01c2a9c6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217d4ba063e7433ba395e73fae796081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:06<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gemma_2b_it_rmu_100\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "loss_added_list = []\n",
    "\n",
    "tlens_model_name = \"google/gemma-2b-it\"\n",
    "model_names = ['gemma_2b_it_rmu_6', 'gemma_2b_it_rmu_10', 'gemma_2b_it_rmu_30', 'gemma_2b_it_rmu_60', 'gemma_2b_it_rmu_100']\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    hf_model_name = \"eoinf/\" + model_name\n",
    "    tmodel = AutoModelForCausalLM.from_pretrained(hf_model_name, torch_dtype='auto') #.to(\"cuda\")\n",
    "\n",
    "    rmu_model = HookedTransformer.from_pretrained(tlens_model_name, hf_model=tmodel, tokenizer=tokenizer)\n",
    "    \n",
    "    metrics = calculate_wmdp_bio_metrics(rmu_model,\n",
    "                                        question_subset=None,\n",
    "                                        question_subset_file=\"../data/wmdp-bio_gemma_2b_it_correct.csv\",\n",
    "                                        permutations=None)\n",
    "    \n",
    "    metrics_list.append(metrics)\n",
    "    \n",
    "    print(\"done metrics\")\n",
    "    \n",
    "    loss_added = get_loss_added_rmu_model(rmu_model, base_model, activation_store, n_batch=20)\n",
    "    \n",
    "    loss_added_list.append(loss_added)\n",
    "    print(\"done\", model_name)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.006862831301987171,\n",
       " -0.006507921032607555,\n",
       " -0.0032344937790185213,\n",
       " -0.0025219798553735018,\n",
       " 0.24644620716571808]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0].mean().item() for x in loss_added_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9941860437393188,\n",
       " 0.9941860437393188,\n",
       " 0.6104651093482971,\n",
       " 0.39534884691238403,\n",
       " 0.36627906560897827]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['mean_correct'] for x in metrics_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9896829128265381,\n",
       " 0.9899671077728271,\n",
       " 0.6090828776359558,\n",
       " 0.3412681818008423,\n",
       " 0.26022282242774963]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['mean_predicted_prob_of_correct_answers'] for x in metrics_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1feba23774f462db8629a3634ee27e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 688/688 [02:17<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gemma_2b_it_rmu_60\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "metrics_list2 = []\n",
    "loss_added_list2 = []\n",
    "\n",
    "tlens_model_name = \"google/gemma-2b-it\"\n",
    "model_names = ['gemma_2b_it_rmu_60']\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    hf_model_name = \"eoinf/\" + model_name\n",
    "    tmodel = AutoModelForCausalLM.from_pretrained(hf_model_name, torch_dtype='auto') #.to(\"cuda\")\n",
    "\n",
    "    rmu_model = HookedTransformer.from_pretrained(tlens_model_name, hf_model=tmodel, tokenizer=tokenizer)\n",
    "    \n",
    "    metrics = calculate_wmdp_bio_metrics(rmu_model,\n",
    "                                        question_subset=None,\n",
    "                                        question_subset_file=\"../data/wmdp-bio_gemma_2b_it_correct.csv\",\n",
    "                                        permutations=all_permutations)\n",
    "    \n",
    "    metrics_list2.append(metrics)\n",
    "    \n",
    "    print(\"done metrics\")\n",
    "    \n",
    "    loss_added = get_loss_added_rmu_model(rmu_model, base_model, activation_store, n_batch=20)\n",
    "    \n",
    "    loss_added_list2.append(loss_added)\n",
    "    print(\"done\", model_name)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_correct': 0.39534884691238403,\n",
       " 'total_correct': 1632,\n",
       " 'is_correct': array([0., 1., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " 'output_probs': array([[2.1784686e-01, 6.4719208e-03, 6.2269843e-01, 1.4312220e-01],\n",
       "        [2.3252216e-01, 4.2762435e-03, 6.5412521e-01, 9.7980998e-02],\n",
       "        [2.3325740e-01, 7.9972176e-03, 6.0639840e-01, 1.4238898e-01],\n",
       "        ...,\n",
       "        [9.9766374e-01, 1.1716344e-06, 1.1497475e-05, 2.0278682e-07],\n",
       "        [9.9772125e-01, 4.6575028e-06, 7.3904243e-07, 1.6371770e-07],\n",
       "        [9.9780995e-01, 3.0797348e-06, 4.3868951e-07, 8.7680355e-08]],\n",
       "       dtype=float32),\n",
       " 'actual_answers': array([3, 2, 3, ..., 2, 1, 1]),\n",
       " 'predicted_answers': array([2, 2, 2, ..., 0, 0, 0]),\n",
       " 'predicted_probs': array([0.6226984 , 0.6541252 , 0.6063984 , ..., 0.99766374, 0.99772125,\n",
       "        0.99780995], dtype=float32),\n",
       " 'predicted_probs_of_correct_answers': array([1.4312220e-01, 6.5412521e-01, 1.4238898e-01, ..., 1.1497475e-05,\n",
       "        4.6575028e-06, 3.0797348e-06], dtype=float32),\n",
       " 'mean_predicted_prob_of_correct_answers': 0.3533848524093628}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_list2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14534883720930233"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(metrics_list2[0]['is_correct'].reshape(-1, 24).mean(axis=1) == 1).astype(float).sum()/172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "all_permutations = list(permutations([0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "\n",
    "\n",
    "# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# model_name = \"../wmdp/models/zephyr_rmu_30\"\n",
    "model_name = \"../wmdp/models/gemma_2b_it_rmu_30\"\n",
    "# model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto') #.to(\"cuda\")\n",
    "\n",
    "tlens_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tlens_model_name = \"google/gemma-2b-it\"\n",
    "print(\"starting\")\n",
    "rmu_model = HookedTransformer.from_pretrained(tlens_model_name, hf_model=tmodel, tokenizer=tokenizer)\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
