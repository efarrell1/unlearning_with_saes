{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sae.sparse_autoencoder import load_saved_sae\n",
    "from sae.metrics import model_store_from_sae\n",
    "from unlearning.metrics import convert_wmdp_data_to_prompt, get_output_probs_abcd\n",
    "from unlearning.tool import UnlearningConfig, SAEUnlearningTool, MCQ_ActivationStoreAnalysis\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49b83a223de4f42aa72d756d5ae7d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# resid pre 9\n",
    "REPO_ID = \"eoinf/unlearning_saes\"\n",
    "FILENAME = \"jolly-dream-40/sparse_autoencoder_gemma-2b-it_blocks.9.hook_resid_pre_s16384_127995904.pt\"\n",
    "\n",
    "\n",
    "filename = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "sae = load_saved_sae(filename)\n",
    "\n",
    "model = model_store_from_sae(sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"high_school_us_history\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'subject', 'choices', 'answer'],\n",
       "    num_rows: 204\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 816/816 [08:18<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "answers = [x['answer'] for x in dataset]\n",
    "questions = [x['question'] for x in dataset]\n",
    "choices_list = [x['choices'] for x in dataset]\n",
    "\n",
    "permutations = list(itertools.permutations(range(4)))\n",
    "pre_question = \"The following are multiple choice questions (with answers) about history.\\n\"\n",
    "\n",
    "if permutations is None:\n",
    "    prompts = [convert_wmdp_data_to_prompt(question, choices, prompt_format=None, pre_question=pre_question) for question, choices in zip(questions, choices_list)]\n",
    "else:\n",
    "    prompts = [[convert_wmdp_data_to_prompt(question, choices, prompt_format=None, permute_choices=p, pre_question=pre_question) for p in permutations]\n",
    "                for question, choices in zip(questions, choices_list)]\n",
    "    prompts = [item for sublist in prompts for item in sublist]\n",
    "    \n",
    "    answers = [[p.index(answer) for p in permutations] for answer in answers]\n",
    "    answers = [item for sublist in answers for item in sublist]\n",
    "\n",
    "\n",
    "actual_answers = answers\n",
    "\n",
    "batch_size = np.minimum(len(prompts), 6)\n",
    "n_batches = len(prompts) // batch_size\n",
    "\n",
    "output_probs = get_output_probs_abcd(model, prompts, batch_size=batch_size, n_batches=n_batches)\n",
    "\n",
    "predicted_answers = output_probs.argmax(dim=1)\n",
    "predicted_probs = output_probs.max(dim=1)[0]\n",
    "\n",
    "n_predicted_answers = len(predicted_answers)\n",
    "\n",
    "actual_answers = torch.tensor(actual_answers)[:n_predicted_answers].to(\"cuda\")\n",
    "\n",
    "predicted_prob_of_correct_answers = output_probs[torch.arange(len(actual_answers)), actual_answers]\n",
    "\n",
    "is_correct = (actual_answers == predicted_answers).to(torch.float)\n",
    "mean_correct = is_correct.mean()\n",
    "\n",
    "metrics = {}\n",
    "metrics['mean_correct'] = float(mean_correct.item())\n",
    "metrics['total_correct'] = int(np.sum(is_correct.cpu().numpy()))\n",
    "metrics['is_correct'] =  is_correct.cpu().numpy()\n",
    "\n",
    "metrics['output_probs'] = output_probs.cpu().numpy()\n",
    "metrics['actual_answers'] = actual_answers.cpu().numpy()\n",
    "\n",
    "metrics['predicted_answers'] = predicted_answers.cpu().numpy()\n",
    "metrics['predicted_probs'] = predicted_probs.cpu().numpy()\n",
    "metrics['predicted_probs_of_correct_answers'] = predicted_prob_of_correct_answers.cpu().numpy()\n",
    "metrics['mean_predicted_prob_of_correct_answers'] = float(np.mean(predicted_prob_of_correct_answers.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of unlearning.metrics failed: Traceback (most recent call last):\n",
      "  File \"/root/unlearning/.venv/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/root/unlearning/.venv/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/root/unlearning/unlearning/metrics.py\", line 174\n",
      "    return partial(calculate_mmlu_metrics, dataset=)\n",
      "                                                   ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_correct': 0.45098039507865906,\n",
       " 'total_correct': 2208,\n",
       " 'is_correct': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " 'output_probs': array([[9.90619138e-03, 2.60861598e-05, 9.87313449e-01, 1.37895859e-05],\n",
       "        [5.78177301e-03, 1.51714175e-05, 1.58104289e-04, 9.92989182e-01],\n",
       "        [1.36140501e-03, 9.96114135e-01, 2.17192246e-05, 1.46499167e-06],\n",
       "        ...,\n",
       "        [2.73208789e-05, 9.96995568e-01, 1.07833091e-06, 7.05016930e-07],\n",
       "        [8.43842827e-06, 4.11083221e-08, 1.80612716e-07, 9.95275259e-01],\n",
       "        [1.58713825e-04, 6.65444486e-06, 9.95751381e-01, 2.11723200e-05]],\n",
       "       dtype=float32),\n",
       " 'actual_answers': array([3, 2, 3, ..., 2, 1, 1]),\n",
       " 'predicted_answers': array([2, 3, 1, ..., 1, 3, 2]),\n",
       " 'predicted_probs': array([0.98731345, 0.9929892 , 0.99611413, ..., 0.99699557, 0.99527526,\n",
       "        0.9957514 ], dtype=float32),\n",
       " 'predicted_probs_of_correct_answers': array([1.3789586e-05, 1.5810429e-04, 1.4649917e-06, ..., 1.0783309e-06,\n",
       "        4.1108322e-08, 6.6544449e-06], dtype=float32),\n",
       " 'mean_predicted_prob_of_correct_answers': 0.4482101798057556}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find a mmlu question that the model can answer correctly no matter which permutation of the choices is\n",
    "# used. Then use it as a control when searching for features\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15,  20,  24,  28,  34,  45,  49,  61,  63,  68,  69,  70,  72,\n",
       "        86,  95,  97,  99, 111, 112, 122, 123, 132, 145, 151, 173, 174,\n",
       "       175, 177, 182, 185, 193])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_permutations_correct = metrics['is_correct'].reshape(-1, 24).mean(axis=1) == 1\n",
    "\n",
    "# get index that is True\n",
    "all_permutations_correct_index = np.where(all_permutations_correct)[0]\n",
    "all_permutations_correct_index\n",
    "\n",
    "# array([ 15,  20,  24,  28,  34,  45,  49,  61,  63,  68,  69,  70,  72,\n",
    "#         86,  95,  97,  99, 111, 112, 122, 123, 132, 145, 151, 173, 174,\n",
    "#        175, 177, 182, 185, 193])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of unlearning.metrics failed: Traceback (most recent call last):\n",
      "  File \"/root/unlearning/.venv/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/root/unlearning/.venv/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/root/unlearning/unlearning/metrics.py\", line 178\n",
      "    return partial(calculate_mmlu_metrics, dataset=)\n",
      "                                                   ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[185]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "[np.random.choice(all_permutations_correct_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False,  True, False, False, False,  True, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "        True, False, False, False, False,  True,  True,  True, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False,  True, False,  True, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True,  True,  True, False,  True, False, False,\n",
       "       False, False,  True, False, False,  True, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_permutations_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about history.\n",
      "This question refers to the following information.\n",
      "\"We conclude that, in the field of public education, the doctrine of \"separate but equal\" has no place. Separate educational facilities are inherently unequal. Therefore, we hold that the plaintiffs and others similarly situated for whom the actions have been brought are, by reason of the segregation complained of, deprived of the equal protection of the laws guaranteed by the Fourteenth Amendment.\"\n",
      "Brown v. Board of Education, 1954\n",
      "Desegregation of schools was, in part, a response to unfulfilled promises from which of the following initiatives?\n",
      "A. Reconstruction\n",
      "B. The Square Deal\n",
      "C. The New Deal\n",
      "D. The Great Society\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = prompts[all_permutations_correct_index[10]]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
