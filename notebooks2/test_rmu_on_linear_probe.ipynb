{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from sae.train import ModelTrainer\n",
    "from sae.config import create_config, Config\n",
    "from sae.utils import get_blog_checkpoint, get_blog_sparsity, create_lineplot_histogram\n",
    "\n",
    "from unlearning.metrics import calculate_MCQ_metrics, calculate_wmdp_bio_metrics_hf, get_loss_added_rmu_model\n",
    "from unlearning.tool import get_basic_gemma_2b_it_layer9_act_store\n",
    "from unlearning.var import gemma_2b_it_rmu_model_names\n",
    "from unlearning.metrics import all_permutations\n",
    "\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from sae.metrics import compute_metrics_post_by_text\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "\n",
    "from sae.sparse_autoencoder import load_saved_sae\n",
    "from sae.metrics import model_store_from_sae\n",
    "from unlearning.metrics import convert_wmdp_data_to_prompt\n",
    "from unlearning.tool import UnlearningConfig, SAEUnlearningTool, MCQ_ActivationStoreAnalysis\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "from unlearning.intervention import anthropic_remove_resid_SAE_features, remove_resid_SAE_features, anthropic_clamp_resid_SAE_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Hugging Face model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94746b6649764552b6cfc4197feace36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting Hugging Face model\")\n",
    "\n",
    "hf_model_name = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(hf_model_name, torch_dtype='auto')\n",
    "\n",
    "transformer_lens_model_name = \"google/gemma-2b-it\"\n",
    "base_model = HookedTransformer.from_pretrained(transformer_lens_model_name, hf_model=hf_model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c3bfb2a45845a7983ced59474d1252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hf_model_name = 'eoinf/gemma_2b_it_rmu_s60_a1000'\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(hf_model_name, torch_dtype='auto') #.to(\"cuda\")\n",
    "model = HookedTransformer.from_pretrained(transformer_lens_model_name, hf_model=hf_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 172 questions that the model can answer correctly in any permutation\n",
    "filename = '../data/wmdp-bio_gemma_2b_it_correct.csv'\n",
    "correct_question_ids = np.genfromtxt(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unlearning.metrics import all_permutations\n",
    "\n",
    "dataset = load_dataset(\"cais/wmdp\", \"wmdp-bio\")\n",
    "\n",
    "prompts = [\n",
    "    convert_wmdp_data_to_prompt(dataset['test'][i]['question'], dataset['test'][i]['choices'], prompt_format=None, permute_choices=p)\n",
    "    for i in range(len(dataset['test'])) \n",
    "    for p in all_permutations\n",
    "    if i in correct_question_ids\n",
    "    \n",
    "]\n",
    "\n",
    "answers = [\n",
    "    p.index(dataset['test'][i]['answer'])\n",
    "    for i in range(len(dataset['test'])) \n",
    "    for p in all_permutations\n",
    "    if i in correct_question_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProbeTrainingArgs():\n",
    "    layer: int = 12\n",
    "    options: int = 4\n",
    "    device: str = \"cuda\"\n",
    "    \n",
    "    # Standard training hyperparams\n",
    "    max_epochs: int = 8\n",
    "    \n",
    "    # Hyperparams for optimizer\n",
    "    batch_size: int = 4\n",
    "    lr: float = 1e-4\n",
    "    betas: tuple[float, float] = (0.9, 0.99)\n",
    "    wd: float = 0.01\n",
    "    \n",
    "    # Saving & logging\n",
    "    probe_name: str = \"main_linear_probe\"\n",
    "    wandb_project: str | None = 'wmdp-probe'\n",
    "    wandb_name: str | None = None\n",
    "    \n",
    "    # prompts and ans\n",
    "    prompts: list[str] = None\n",
    "    answers: list[int] = None\n",
    "    \n",
    "    # Code to get randomly initialized probe\n",
    "    def setup_linear_probe(self, model: HookedTransformer):\n",
    "        linear_probe = torch.randn(\n",
    "            model.cfg.d_model, self.options, requires_grad=False, device=self.device\n",
    "        ) / np.sqrt(model.cfg.d_model)\n",
    "        linear_probe.requires_grad = True\n",
    "        return linear_probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myeutong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ebc4d1f51c48e1be2ac27d7b10f83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112689226865768, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/unlearning/yeutong_notebooks/wandb/run-20240718_000939-a5rnztfw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yeutong/wmdp-probe/runs/a5rnztfw' target=\"_blank\">rose-donkey-38</a></strong> to <a href='https://wandb.ai/yeutong/wmdp-probe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yeutong/wmdp-probe' target=\"_blank\">https://wandb.ai/yeutong/wmdp-probe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yeutong/wmdp-probe/runs/a5rnztfw' target=\"_blank\">https://wandb.ai/yeutong/wmdp-probe/runs/a5rnztfw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [02:29<00:00,  1.73it/s]\n",
      "100%|██████████| 258/258 [02:30<00:00,  1.72it/s]\n",
      "100%|██████████| 258/258 [02:30<00:00,  1.72it/s]\n",
      "100%|██████████| 258/258 [02:29<00:00,  1.72it/s]\n",
      "100%|██████████| 258/258 [02:30<00:00,  1.72it/s]\n",
      "100%|██████████| 258/258 [02:29<00:00,  1.72it/s]\n",
      "100%|██████████| 258/258 [02:29<00:00,  1.72it/s]\n",
      "100%|██████████| 258/258 [02:29<00:00,  1.72it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45967a6ecbd4a5eb977a52cf211a92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▄▁▄▂▅▄█▅▃▅▇▄▃▂▅▄▅▄▆▇▂▄▅▅▅▃▄▅▄▂▄▄▅▃▅▄▃▂█▃</td></tr><tr><td>loss</td><td>█▇▆▇▆█▅▅▆▆▄▆▆▇▆▅▄▅▁▄▇▆▇▄▅▅▅▅▇▇▅▆▄▇▅▆▆▅▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.4375</td></tr><tr><td>loss</td><td>1.15351</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rose-donkey-38</strong> at: <a href='https://wandb.ai/yeutong/wmdp-probe/runs/a5rnztfw' target=\"_blank\">https://wandb.ai/yeutong/wmdp-probe/runs/a5rnztfw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240718_000939-a5rnztfw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LinearProbeTrainer:\n",
    "    def __init__(self, model: HookedTransformer, args: ProbeTrainingArgs):\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.linear_probe = args.setup_linear_probe(model)\n",
    "        self.prompts = args.prompts\n",
    "        self.answers = args.answers\n",
    "        self.early_stopping = False\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.prompts, self.answers = self.prompts.copy(), self.answers.copy()\n",
    "        zipped = list(zip(self.prompts, self.answers))\n",
    "        np.random.shuffle(zipped)\n",
    "        self.prompts, self.answers = zip(*zipped)\n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        self.step = 0\n",
    "        wandb.init(project=args.wandb_project, name=args.wandb_name, config=args)\n",
    "\n",
    "        optimizer = torch.optim.AdamW([self.linear_probe], lr=self.args.lr, betas=self.args.betas, weight_decay=self.args.wd)\n",
    "        \n",
    "        self.shuffle()\n",
    "        \n",
    "        self.accuracy_history = []\n",
    "        \n",
    "        for epoch in range(args.max_epochs):\n",
    "            \n",
    "            for i in tqdm(range(0, len(self.prompts), self.args.batch_size)):\n",
    "                prompt_batch = self.prompts[i: i + self.args.batch_size]\n",
    "                answers_batch = self.answers[i: i + self.args.batch_size]\n",
    "                current_batch_size = len(prompt_batch)\n",
    "                token_batch = model.to_tokens(prompt_batch, padding_side=\"right\").to(\"cuda\")\n",
    "                \n",
    "                token_lens = [len(model.to_tokens(x)[0]) for x in prompt_batch]\n",
    "                next_token_indices = torch.tensor([x - 1 for x in token_lens]).to(\"cuda\")\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    _, cache = model.run_with_cache(\n",
    "                        token_batch.to(self.args.device),\n",
    "                        return_type=None,\n",
    "                        names_filter=lambda name: name.endswith(\"resid_post\")\n",
    "                    )\n",
    "                    resid_post: Float[Tensor, \"batch d_model\"] = cache[\"resid_post\", self.args.layer][torch.arange(current_batch_size), next_token_indices]\n",
    "                \n",
    "                resid_post = resid_post.clone()\n",
    "\n",
    "                probe_out = einops.einsum(\n",
    "                    resid_post,\n",
    "                    self.linear_probe,\n",
    "                    \"batch d_model, d_model options -> batch options\",\n",
    "                )\n",
    "                \n",
    "                # print(probe_out)\n",
    "                probe_log_probs = probe_out.log_softmax(-1)\n",
    "                \n",
    "                # print(probe_log_probs)\n",
    "                # print(answers_batch)\n",
    "                probe_correct_log_probs = probe_log_probs[torch.arange(current_batch_size), answers_batch]\n",
    "                # print(probe_correct_log_probs)\n",
    "\n",
    "                accuracy = (probe_log_probs.argmax(dim=-1) == torch.tensor(answers_batch).to(\"cuda\")).to(float).mean()\n",
    "                # print(accuracy)\n",
    "                \n",
    "                loss = -probe_correct_log_probs.mean()\n",
    "                # print(loss)\n",
    "                loss.backward()\n",
    "                wandb.log({\"loss\": loss.item(), \"accuracy\": accuracy.item()}, step=self.step)\n",
    "                self.step += 1\n",
    "                \n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # add early stopping, if accuracy is 1.0 for 5 steps, break\n",
    "                self.accuracy_history.append(accuracy.item())\n",
    "                \n",
    "                if len(self.accuracy_history) > 5 and all([x == 1.0 for x in self.accuracy_history[-5:]]):\n",
    "                    self.early_stopping = True\n",
    "                    break\n",
    "                \n",
    "            if self.early_stopping:\n",
    "                break\n",
    "                \n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "layer = 12\n",
    "batch_size = 16\n",
    "args = ProbeTrainingArgs(prompts=prompts, answers=answers, layer=layer, batch_size=batch_size)\n",
    "trainer = LinearProbeTrainer(model, args)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
